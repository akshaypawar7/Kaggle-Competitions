{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Loading neccesary packages:\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\n\n#\n\nfrom scipy import stats\nfrom scipy.stats import skew, boxcox_normmax, norm\nfrom scipy.special import boxcox1p\n\n#\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n# experiment class\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\n\n#\n\nimport matplotlib.gridspec as gridspec\nfrom matplotlib.ticker import MaxNLocator\n\n#\n\nimport warnings\npd.options.display.max_columns = 250\npd.options.display.max_rows = 250\nwarnings.filterwarnings('ignore')\nplt.style.use('fivethirtyeight')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-18T04:28:22.321565Z","iopub.execute_input":"2022-02-18T04:28:22.322056Z","iopub.status.idle":"2022-02-18T04:28:23.772589Z","shell.execute_reply.started":"2022-02-18T04:28:22.321946Z","shell.execute_reply":"2022-02-18T04:28:23.771403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Meeting the data\nWe're going to start by loading the data and taking first look on it as usual. For the column names we have great dictionary file in our dataset location so we can get familiar with them in no time.","metadata":{}},{"cell_type":"code","source":"# Loading datasets.\n\ntrain = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\ntest = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')","metadata":{"execution":{"iopub.status.busy":"2022-02-18T04:28:23.774752Z","iopub.execute_input":"2022-02-18T04:28:23.775024Z","iopub.status.idle":"2022-02-18T04:28:23.861001Z","shell.execute_reply.started":"2022-02-18T04:28:23.774992Z","shell.execute_reply":"2022-02-18T04:28:23.860084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train","metadata":{"execution":{"iopub.status.busy":"2022-02-18T04:28:23.863043Z","iopub.execute_input":"2022-02-18T04:28:23.863336Z","iopub.status.idle":"2022-02-18T04:28:23.964435Z","shell.execute_reply.started":"2022-02-18T04:28:23.863303Z","shell.execute_reply":"2022-02-18T04:28:23.963403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test","metadata":{"execution":{"iopub.status.busy":"2022-02-18T04:28:23.966912Z","iopub.execute_input":"2022-02-18T04:28:23.967307Z","iopub.status.idle":"2022-02-18T04:28:24.069446Z","shell.execute_reply.started":"2022-02-18T04:28:23.967262Z","shell.execute_reply":"2022-02-18T04:28:24.068396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.describe()","metadata":{"execution":{"iopub.status.busy":"2022-02-18T04:28:24.070825Z","iopub.execute_input":"2022-02-18T04:28:24.071069Z","iopub.status.idle":"2022-02-18T04:28:24.191236Z","shell.execute_reply.started":"2022-02-18T04:28:24.07104Z","shell.execute_reply":"2022-02-18T04:28:24.190089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Id column looks useless we can safely drop it from both. I'm going to save our target (SalePrice) on different variable so we can use it in future.","metadata":{}},{"cell_type":"code","source":"# Dropping unnecessary Id column.\n\ntrain.drop('Id', axis=1, inplace=True)\ntest.drop('Id', axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-18T04:28:24.192518Z","iopub.execute_input":"2022-02-18T04:28:24.192755Z","iopub.status.idle":"2022-02-18T04:28:24.202344Z","shell.execute_reply.started":"2022-02-18T04:28:24.192725Z","shell.execute_reply":"2022-02-18T04:28:24.201062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Backing up target variables and dropping them from train data.\n\ny = train.SalePrice.reset_index(drop=True)\ntrain_features = train.drop('SalePrice', axis=1)\ntest_features = test","metadata":{"execution":{"iopub.status.busy":"2022-02-18T04:28:24.204297Z","iopub.execute_input":"2022-02-18T04:28:24.204618Z","iopub.status.idle":"2022-02-18T04:28:24.216417Z","shell.execute_reply.started":"2022-02-18T04:28:24.204577Z","shell.execute_reply":"2022-02-18T04:28:24.215499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Analysis Time!\nOk the short inspection at the beginning give us some hints how should we move from here. I'm going to play with the data we have while analysing the data at the same time. With this way I hope we can get the data in better shape while digging deeper into it.\n\nWe're going to start with basic correlation table here. I dropped the top part since it's just mirror of the other part below. With this table we can understand some linear relations between different features.\n\n### Observations:\n* There's strong relation between overall quality of the houses and their sale prices.\n* Again above grade living area seems strong indicator for sale price.\n* Garage features, number of baths and rooms, how old the building is etc. also having effect on the price on various levels too.\n* There are some obvious relations we gonna pass like total square feet affecting how many rooms there are or how many cars can fit into a garage vs. garage area etc.\n* Overall condition of the house seems less important on the pricing, it's interesting and worth digging.","metadata":{}},{"cell_type":"code","source":"# Display numerical correlations (pearson) between features on heatmap.\n\nsns.set(font_scale=1.1)\ncorrelation_train = train.corr()\nmask = np.triu(correlation_train.corr())\nplt.figure(figsize=(20, 20))\nsns.heatmap(correlation_train,\n            annot=True,\n            fmt='.1f',\n            cmap='coolwarm',\n            square=True,\n            mask=mask,\n            linewidths=1,\n            cbar=False)\n\nplt.show()\n\n# delete unnecesory varieble \ndel correlation_train, mask\n","metadata":{"execution":{"iopub.status.busy":"2022-02-18T04:28:24.218448Z","iopub.execute_input":"2022-02-18T04:28:24.218819Z","iopub.status.idle":"2022-02-18T04:28:27.60536Z","shell.execute_reply.started":"2022-02-18T04:28:24.218774Z","shell.execute_reply":"2022-02-18T04:28:27.604499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **I'm going to merge the datasets here before we start editing it so we don't have to do these operations twice. Let's call it features since it has features only. So our data has 2919 observations and 79 features to begin with...**","metadata":{}},{"cell_type":"code","source":"# Merging train test features for engineering.\n\nfeatures = pd.concat([train_features, test_features]).reset_index(drop=True)\nprint(features.shape)","metadata":{"execution":{"iopub.status.busy":"2022-02-18T04:28:27.606696Z","iopub.execute_input":"2022-02-18T04:28:27.607059Z","iopub.status.idle":"2022-02-18T04:28:27.636518Z","shell.execute_reply.started":"2022-02-18T04:28:27.607023Z","shell.execute_reply":"2022-02-18T04:28:27.635734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Missing Data\nAlright, first of all we need detect missing values, then wee need to get rid of them for the next steps of our work. So let's list our missing values and visualize them:","metadata":{}},{"cell_type":"code","source":"def missing_percentage(df):\n    \n    \"\"\"A function for returning missing ratios.\"\"\"\n    total = df.isnull().sum().sort_values(ascending=False)\n    \n    return pd.concat([total, (total / len(df) * 100)], axis=1, keys=['Total', 'Percent'])[total!=0]","metadata":{"execution":{"iopub.status.busy":"2022-02-18T04:28:27.638831Z","iopub.execute_input":"2022-02-18T04:28:27.639272Z","iopub.status.idle":"2022-02-18T04:28:27.646069Z","shell.execute_reply.started":"2022-02-18T04:28:27.639237Z","shell.execute_reply":"2022-02-18T04:28:27.645202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **That's quite a lot! No need to panic though we got this. If you look at the data description given to us we can see that most of these missing data actually not missing, it's just means house doesn't have that specific feature, we can fix that easily...**","metadata":{}},{"cell_type":"code","source":"# Checking 'NaN' values.\n\nmissing = missing_percentage(features)\n\nfig, ax = plt.subplots(figsize=(20, 5))\nsns.barplot(x=missing.index, y='Percent', data=missing, palette='Reds_r')\nplt.xticks(rotation=90)\n\ndisplay(missing.T.style.background_gradient(cmap='Reds', axis=1))\n\ndel missing","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-18T04:28:27.647804Z","iopub.execute_input":"2022-02-18T04:28:27.64878Z","iopub.status.idle":"2022-02-18T04:28:28.413162Z","shell.execute_reply.started":"2022-02-18T04:28:27.648726Z","shell.execute_reply":"2022-02-18T04:28:28.411905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ok this is how we gonna fix most of the missing data:\n1. First we fill the NaN's in the columns where they mean 'None' so we gonna replace them with that,\n2. Then we fill numerical columns where missing values indicating there is no parent feature to measure, so we replace them with 0's.\n3. Even with these there are some actual missing data, by checking general trends of these features we can fill them with most frequent value(with mode).\n4. MSZoning part is little bit tricky I choose to fill them with most common type of the related MSSubClass type. It's not perfect but at least we decrease randomness a little bit.\n4. Again we fill the Lot Frontage with similar approach.","metadata":{}},{"cell_type":"code","source":"\n# Features which numerical on data but should be treated as category:\n\nfor i in ['MSSubClass', 'YrSold', 'MoSold']:\n    features[i] = features[i].astype(str)\n\n\n# List of 'NaN' including columns where NaN's mean's none.\n\nnone_cols = [\n    'Alley', 'PoolQC', 'MiscFeature', 'Fence', 'FireplaceQu', 'GarageType',\n    'GarageFinish', 'GarageQual', 'GarageCond', 'BsmtQual', 'BsmtCond',\n    'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'MasVnrType'\n]\n\n# List of 'NaN' including columns where NaN's mean's 0.\n\nzero_cols = [\n    'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath',\n    'BsmtHalfBath', 'GarageYrBlt', 'GarageArea', 'GarageCars', 'MasVnrArea'\n]\n\n# List of 'NaN' including columns where NaN's actually missing gonna replaced with mode.\n\nmost_cols = [\n    'Electrical', 'Exterior1st', 'Exterior2nd', 'Functional', 'KitchenQual',\n    'SaleType', 'Utilities', 'MSZoning'\n]\n\n# Multivariate feature imputation method\n# for ref => https://scikit-learn.org/stable/modules/impute.html#univariate-feature-imputation\n\nregg_cols = ['LotFrontage']#,'MSZoning']\n","metadata":{"execution":{"iopub.status.busy":"2022-02-18T04:28:28.414583Z","iopub.execute_input":"2022-02-18T04:28:28.414905Z","iopub.status.idle":"2022-02-18T04:28:28.436663Z","shell.execute_reply.started":"2022-02-18T04:28:28.414873Z","shell.execute_reply":"2022-02-18T04:28:28.435938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# impute missing value with Column transformer\n\nmissing_value_preprocessor = ColumnTransformer(\n    transformers=[\n        # imputation\n        ('none_imputer', SimpleImputer(fill_value= 'none', strategy='constant'), none_cols),\n        ('zero_imputer', SimpleImputer(fill_value= 0, strategy='constant'), zero_cols),\n        ('most_imputer', SimpleImputer(strategy='most_frequent'), most_cols),\n        # experimental class imputation => Multivariate feature imputation\n        ('regg_features', IterativeImputer(max_iter=10, random_state=0), regg_cols),\n    ],\n    remainder = 'passthrough',\n)","metadata":{"execution":{"iopub.status.busy":"2022-02-18T04:28:28.437708Z","iopub.execute_input":"2022-02-18T04:28:28.438468Z","iopub.status.idle":"2022-02-18T04:28:28.445128Z","shell.execute_reply.started":"2022-02-18T04:28:28.438426Z","shell.execute_reply":"2022-02-18T04:28:28.444115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = pd.DataFrame(\n    missing_value_preprocessor.fit_transform(features),\n    columns = features.columns\n)","metadata":{"execution":{"iopub.status.busy":"2022-02-18T04:28:28.446889Z","iopub.execute_input":"2022-02-18T04:28:28.447399Z","iopub.status.idle":"2022-02-18T04:28:28.537092Z","shell.execute_reply.started":"2022-02-18T04:28:28.447349Z","shell.execute_reply":"2022-02-18T04:28:28.536146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering\nOk this is the part where we dig deeper into our completed dataset. There are no missing values so we're good to go!\n","metadata":{}},{"cell_type":"code","source":"def show_box(y, df):\n    \n    '''A function for displaying categorical variables.'''\n    \n    fig, axes = plt.subplots(14, 3, figsize=(25, 80))\n    axes = axes.flatten()\n    \n    for i, j in zip(df.select_dtypes(include=['object']).columns, axes):\n        \n        sortd = df.groupby([i])[y].median().sort_values(ascending=False)\n        sns.boxplot(x=i,\n                    y=y,\n                    data=df,\n                    palette='plasma',\n                    order=sortd.index,\n                    ax=j)\n        j.tick_params(labelrotation=45)\n        j.yaxis.set_major_locator(MaxNLocator(nbins=18))\n\n        plt.tight_layout()\n\n","metadata":{"execution":{"iopub.status.busy":"2022-02-18T04:28:28.538712Z","iopub.execute_input":"2022-02-18T04:28:28.538978Z","iopub.status.idle":"2022-02-18T04:28:28.547269Z","shell.execute_reply.started":"2022-02-18T04:28:28.538948Z","shell.execute_reply":"2022-02-18T04:28:28.546125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Categorical Data\nWe already checked some of the numerical features with correlation heatmap but what about categorical values? We want to see relations between categorical data and sale price. Boxplots seems decent way to inspect this type of relation. We're also going to sort them by the median value of that group so we can see the importances in descending order.","metadata":{}},{"cell_type":"code","source":"# Displaying sale prices vs. categorical values:\n\nshow_box('SalePrice', train)","metadata":{"execution":{"iopub.status.busy":"2022-02-18T04:28:28.549155Z","iopub.execute_input":"2022-02-18T04:28:28.549504Z","iopub.status.idle":"2022-02-18T04:29:28.686897Z","shell.execute_reply.started":"2022-02-18T04:28:28.549462Z","shell.execute_reply":"2022-02-18T04:29:28.685932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Numeric Data\nThere are many numeric features the inspect, one of the best ways to see how they effect sale prices is scatter plots. We're also plotting polynomial regression lines to see general trend. With this way we can understand the numerical values and their importance on sale price, also it's really helpful to spot outliers.\n\n### Observations:\n* OverallQual; It's clearly visible that sale price of the house increases with overall quality. This confirms the correlation in first table we did at the beginning. (Pearson corr was 0.8)\n\n* OverallCondition; Looks like overall condition is left skewed where most of the houses are around 5/10 condition. But it doesn't effect the price like quality indicator...\n\n* YearBuilt; Again new buildings are generally expensive than the old ones.\n\n* Basement; General table shows bigger basements are increasing the price but I see some outliers there...\n\n* GrLivArea; This feature is pretty linear but we can spot two outliers effecting this trend. There are some huge area houses with pretty cheap prices, there might be some reason behind it but we better drop them.\n\n* SaleDates; They seem pretty unimportant on sale prices, we can drop them...","metadata":{}},{"cell_type":"code","source":"# Plotting numerical features with polynomial order to detect outliers by eye.\n\ndef show_reg(y, df):\n    fig, axes = plt.subplots(12, 3, figsize=(25, 80))\n    axes = axes.flatten()\n    \n    for i, j in zip(df.select_dtypes(include=['number']).columns, axes):\n\n        sns.regplot(x=i,\n                    y=y,\n                    data=df,\n                    ax=j,\n                    order=3,\n                    ci=None,\n                    color='#e74c3c',\n                    line_kws={'color': 'black'},\n                    scatter_kws={'alpha':0.4})\n        j.tick_params(labelrotation=45)\n        j.yaxis.set_major_locator(MaxNLocator(nbins=10))\n\n        plt.tight_layout()\n\nshow_reg('SalePrice', train)","metadata":{"execution":{"iopub.status.busy":"2022-02-18T04:29:28.688078Z","iopub.execute_input":"2022-02-18T04:29:28.688609Z","iopub.status.idle":"2022-02-18T04:30:13.707038Z","shell.execute_reply.started":"2022-02-18T04:29:28.688571Z","shell.execute_reply":"2022-02-18T04:30:13.705252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = features.convert_dtypes()\nfeatures.select_dtypes(exclude=['string'])\ntrain","metadata":{"execution":{"iopub.status.busy":"2022-02-18T04:30:13.708644Z","iopub.execute_input":"2022-02-18T04:30:13.708898Z","iopub.status.idle":"2022-02-18T04:30:13.894178Z","shell.execute_reply.started":"2022-02-18T04:30:13.708868Z","shell.execute_reply":"2022-02-18T04:30:13.892959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#features.select_dtypes(include=['object'])\n#features.dtypes\nfrom sklearn import set_config\nset_config(display='diagram')   \n# displays HTML representation in a jupyter context\nmissing_value_preprocessor\n","metadata":{"execution":{"iopub.status.busy":"2022-02-18T04:30:13.895487Z","iopub.execute_input":"2022-02-18T04:30:13.895769Z","iopub.status.idle":"2022-02-18T04:30:13.923144Z","shell.execute_reply.started":"2022-02-18T04:30:13.895737Z","shell.execute_reply":"2022-02-18T04:30:13.921857Z"},"trusted":true},"execution_count":null,"outputs":[]}]}